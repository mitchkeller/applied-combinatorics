<section xml:id="s_basics_big-oh">
    <title>The Big <q>Oh</q> and Little <q>Oh</q> Notations</title>
    <p>Let <m>f:\posints\longrightarrow \reals</m> and <m>g:\posints\longrightarrow\reals</m> be functions.  We write <m>f=O(g)</m>, and say <m>f</m> is <q><term>Big Oh</term></q><idx><h>big Oh notation</h></idx> of <m>g</m>, when there is a constant <m>c</m> and an integer <m>n_0</m> so that <m>f(n)\le cg(n)</m> whenever <m>n>n_0</m>.  Although this notation has a long history, we can provide a quite modern justification. If <m>f</m> and <m>g</m> both describe the number of operations required for two algorithms given input size <m>n</m>, then the meaning of <m>f=O(g)</m> is that <m>f</m> is no harder than <m>g</m> when the problem size is large.
    </p>

    <p>We are particularly interested in comparing functions against certain natural benchmarks, <eg />, <m>\log\log n</m>, <m>\log n</m>, <m>\sqrt{n}</m>, <m>n^\alpha</m> where <m>\alpha\lt 1</m>, <m>n</m>, <m>n^2</m>, <m>n^3</m>, <m>n^c</m> where <m>c>1</m> is a constant, <m>n^{\log n}</m>, <m>2^n</m>, <m>n!</m>, <m>2^{n^2}</m>, <etc />
    </p>

    <p>For example, in <xref ref="s_induction_recursion_sorting"  /> we learned that there are sorting algorithms with running time <m>O(n\log n)</m> where <m>n</m> is the number of integers to be sorted. As a second example, we will learn that we can find all shortest paths in an oriented graph on <m>n</m> vertices with non-negative weights on edges with an algorithm having running time <m>O(n^2)</m>. At the other extreme, no one knows whether there is a constant <m>c</m> and an algorithm for determining whether the chromatic number of a graph is at most three which has running time <m>O(n^c)</m>.
    </p>

    <p>It is important to remember that when we write <m>f=O(g)</m>, we are implying in some sense that <m>f</m> is no bigger than <m>g</m>, but it may in fact be much smaller. By contrast, there will be times when we really know that one function dominates another. And we have a second kind of notation to capture this relationship.
    </p>

    <p>Let <m>f:\posints\longrightarrow \reals</m> and <m>g:\posints\longrightarrow\reals</m> be functions with <m>f(n)>0</m> and <m>g(n)>0</m> for all <m>n</m>. We write <m>f=o(g)</m>, and say that <m>f</m> is <q><term>Little oh</term></q><idx><h>little oh notation</h></idx> of <m>g</m>, when <m>\lim_{n\rightarrow\infty}f(n)/g(n)=0</m>.  For example <m>\ln n=o(n^{.2})</m>; <m>n^\alpha=o(n^{\beta})</m> whenever <m>0\lt \alpha\lt \beta</m>; and <m>n^{100}=o(c^n)</m> for every <m>c>1</m>.  In particular, we write <m>f(n)=o(1)</m> when <m>\lim_{n\rightarrow\infty}f(n)=0</m>.
    </p>
</section>
