

<section permid="zhT" xml:id="s_kitchensink_markov-chains">
  <title>Markov Chains</title>
  <introduction permid="XKD">
    <p permid="qyb">
      We begin this section with a motivational example.
      Consider the connected graph on six vertices shown in <xref provisional="fig-markovchain" />.
      The first move is to choose a vertex at random and move there.
      Afterwards, we follow the following recursive procedures.
      If after <m>i</m> moves,
      you are at a vertex <m>x</m> and <m>x</m> has <m>d</m> neighbors,
      choose one of the neighbors at random,
      with each having probability <m>1/d</m> and move there.
      We then attempt to answer questions of the following flavor:

      <ol permid="DRM">
        <li permid="wnn">
          <p permid="Nly">
            For each vertex <m>x</m>,
            let <m>p_{x,m}</m> denote the probability that you are at vertex <m>x</m> after <m>m</m> moves.
            Does <m>\lim_{m\rightarrow\infty}p_{x,m}</m> exist and if so,
            how fast does the sequence converge to this limit?
          </p>
        </li>

        <li permid="cuw">
          <p permid="tsH">
            How many moves must I make in order that the probability that I have walked on every edge in the graph is at least <m>0.999</m>?
          </p>
        </li>
      </ol>
    </p>

    <p permid="WFk">
      This example illustrates the characteristics of an important class of computational and combinatorial problems,
      which are collectively referred to as
      <term>Markov Chains</term>
          <idx><h>Markov chain</h></idx>
      :

      <ol permid="jYV">
        <li permid="IBF">
          <p permid="ZzQ">
            There is a finite set of states <m>S_1</m>, <m>S_2,\dots,S_n</m>,
            and at time <m>i</m>, you are in one of these states.
          </p>
        </li>

        <li permid="oIO">
          <p permid="FGZ">
            If you are in state <m>S_j</m> at time <m>i</m>, then for each
            <m>k=1,2,\dots,n</m>, there is a fixed probability <m>p(j,k)</m>
            (which does not depend on <m>i</m>)
            that you will be in state <m>S_k</m> at time <m>i+1</m>.
          </p>
        </li>
      </ol>
    </p>

    <p permid="CMt">
      The <m>n\times n</m> matrix <m>P</m> whose <m>j,k</m> entry is the probability <m>p(j,k)</m> of moving from state <m>S_j</m> to state <m>S_k</m> is called the
      <term>transition matrix</term>
          <idx><h>matrix</h><h>transition</h></idx>
      of the Markov chain.
      Note that <m>P</m> is a <term>stochastic matrix</term>,
          <idx><h>matrix</h><h>stochastic</h></idx>
      <ie />, all entries are non-negative and all row sums are<nbsp /><m>1</m>.
      Conversely, each square stochastic matrix can be considered as the transition matrix of a Markov chain.
    </p>

    <p permid="iTC">
      For example,
      here is the transition matrix for the graph in <xref provisional="fig-markovchain"  />.
      <men permid="TKE" xml:id="eqn_transition">
        P =\begin{pmatrix}0 \amp  1/4 \amp  1/4 \amp  1/4 \amp  1/4 \amp    0 \\
        1/2 \amp    0 \amp    0 \amp  1/2 \amp    0 \amp    0 \\
        1/3 \amp    0 \amp    0 \amp  1/3 \amp    0 \amp  1/3 \\
        1/3 \amp  1/3 \amp  1/3 \amp    0 \amp    0 \amp    0 \\
        1 \amp    0 \amp    0 \amp    0 \amp    0 \amp    0 \\
        0 \amp    0 \amp    1 \amp    0 \amp    0 \amp    0 \\
        \end{pmatrix}
      </men>
    </p>

    <p permid="PaL">
      A transition matrix <m>P</m> is
      <term>regular</term><idx><h>regular</h><h>transition matrix</h></idx> if there is some integer <m>m</m> for which the matrix <m>P^m</m> has only positive entries.
      Here is a fundamental result from this subject,
      one that is easy to understand but a bit too complex to prove given our space constraints.
    </p>

    <theorem permid="rDu" xml:id="thm_regular">
      <statement>
        <p permid="ejJ">
          Let <m>P</m> be a regular <m>n\times n</m> transition matrix.
          Then there is a row vector
          <m>W=(w_1, w_2,\dots,w_n</m>) of positive real numbers summing to <m>1</m> so that as <m>m</m> tends to infinity,
          each row of <m>P^m</m> tends to <m>W</m>.
          Furthermore, <m>WP=W</m>, and for each <m>i=1,2,\dots,n</m>,
          the value <m>w_i</m> is the limiting probability of being in state <m>S_i</m>.
        </p>
      </statement>
    </theorem>

    <p permid="vhU">
      Given the statement of <xref ref="thm_regular" />,
      the computation of the row vector <m>W</m> can be carried out by eigenvalue techniques that are part of a standard undergraduate linear algebra course.
      For example,
      the transition matrix <m>P</m> displayed in <xref ref="eqn_transition" /> is regular since all entries of <m>P^3</m> are positive.
      Furthermore, for this matrix,
      the row vector <m>W=(5/13, 3/13, 2/13, 2/13, 1/13, 1/13)</m>.
      However, the question involving how fast the convergence of <m>P^m</m> is to this limiting vector is more subtle,
      as is the question as to how long it takes for us to be relatively certain we have made every possible transition.
    </p>
  </introduction>

  <subsection permid="fpc">
    <title>Absorbing Markov Chains</title>
    <p permid="bpd">
      A state <m>S_i</m> in a Markov chain with transition matrix <m>P</m> is
      <term>absorbing</term><idx><h>absorbing</h><h>state of a Markov chain</h></idx> if <m>p_{i,i}=1</m> and
      <m>p_{i,j}=0</m> for all <m>j\neq i</m>,
      <ie />, like the infamous Hotel California,
      once you are in state <m>S_i</m>,
      <q>you can never leave.</q>
    </p>

    <example permid="Lwl">
      <p permid="KqS">
        We modify the transition matrix from <xref ref="eqn_transition"  /> by making states <m>4</m> and <m>5</m> absorbing.
        The revised transition matrix is now:
        <men permid="zRN" xml:id="eqn_absorbing">
          P =\begin{pmatrix}0 \amp  1/4 \amp  1/4 \amp  1/4 \amp  1/4 \amp    0 \\
          1/2 \amp    0 \amp    0 \amp  1/2 \amp    0 \amp    0 \\
          1/3 \amp    0 \amp    0 \amp  1/3 \amp    0 \amp  1/3 \\
          1/3 \amp  1/3 \amp  1/3 \amp    0 \amp    0 \amp    0 \\
          0 \amp    0 \amp    0 \amp    0 \amp    1 \amp    0 \\
          0 \amp    0 \amp    0 \amp    0 \amp    0 \amp    1 \\
          \end{pmatrix}
        </men>
      </p>
    </example>

    <p permid="Hwm">
      Now we might consider the following game.
      Start at one of the four vertices in
      <m>\{1,2,3,4\}</m> and proceed as before,
      making moves by choosing a neighbor at random.
      Vertex <m>4</m> might be considered as an <q>escape</q> point,
      a safe harbor that once reached is never left.
      On the other hand,
      vertex <m>5</m> might be somewhere one meets a hungry tiger and be absorbed in a way not to be detailed here.
    </p>

    <p permid="nDv">
      We say the Markov chain is <term>absorbing</term><idx><h>absorbing</h><h>Markov chain</h></idx> if there is at least one absorbing state and for each state <m>S_j</m> that is not absorbing,
      it is possible to reach an absorbing state<mdash />although it may take many steps to do so.
      Now the kinds of questions we would like to answer are:

      <ol permid="Qge">
        <li permid="UPX">
          <p permid="lOi">
            If we start in non-absorbing state <m>S_i</m>,
            what is the probability of reaching absorbing state <m>S_j</m> (and then being absorbed in that state,
            a question which takes on genuine unpleasantness relative to tigers)?
          </p>
        </li>

        <li permid="AXg">
          <p permid="RVr">
            If we are absorbed in state <m>S_j</m>,
            what is the probability that we started in non-absorbing state <m>S_i</m>?
          </p>
        </li>

        <li permid="hep">
          <p permid="ycA">
            If we start in non-absorbing state <m>S_i</m>,
            what is the expected length of time before we will be absorbed?
          </p>
        </li>
      </ol>
    </p>
  </subsection>

</section>

