<section xml:id="s_recurrence_discussion">
    <title>Discussion</title>
    <p>Yolanda took a sip of coffee <q>I'm glad I paid attention when we were studying vector spaces, bases, and dimension.  All this stuff about solutions for recurrence equations made complete sense. And I can really understand why the professor was making a big deal out of factoring.  We saw it our first semester when we were learning about partial fractions in calculus. And we saw it again with the differential equations stuff. Isn't it really neat to see how it all fits together?</q> All this enthusiasm was too much for Alice who was not having a good day. Bob was more sympathetic, saying <q>Except for the detail about zero as a root of an advancement operator polynomial, I was ok with this chapter.</q> Xing said <q>Here we learned a precise approach that depended only on factoring.  I've been reading on the web and I see that there have been some recent breakthroughs on factoring.</q> Bob jumped back in <q>But even if you can factor like crazy, if you have a large degree polynomial in the advancement operator equation, then you will have lots of initial conditions.  This might be a second major hurdle.</q> Dave mumbled <q>Just do the factoring. The rest is easy.</q> Carlos again was quiet but he knew that Dave was right. Solving big systems of linear equations is relatively easy. The challenge is in the factoring stage.
    </p>

    <p>Despite thinking the material of this chapter was interesting, Bob also wondered if they really needed all of this machinery. <q>Defining a recursive function is easy in almost all programming languages, so why not just use a computer to calculate the values you need?</q><fn>The history of how recursion made its way into ALGOL (and therefore most modern programming languages) involved some intrigue. Maarten van Emden recounts this in a blog post entitled <q><url href="https://vanemden.wordpress.com/2014/06/18/how-recursion-got-into-programming-a-comedy-of-errors-3/">How recursion got into programming: a tale of intrigue, betrayal, and advanced programming-language semantics</url></q>.</fn> Xing started to remark that the techniques of this chapter could provide a good way to understand the growth rate of recursive functions in terms of the big Oh notation of <xref ref="ch_basics" />, but Alice interrupted to propose a programming experiment as something that would raise her spirits. (The chance to prove Bob wrong was probably more motivational than the chance to do some coding, but she didn't want to be <em>too</em> mean.)</p>
    <p>The group decided to take a look at the recurrence in <xref ref="ex_recurrence_gf-nonhomog" />, which they immediately wrote as a recursive function defined on the nonnegative integers by
    <me>r(n) =\begin{cases} 2^n + r(n-1) + 2r(n-2)\amp \text{if }n\geq 2;\\
    1\amp\text{if }n=1;\\
    2\amp\text{if }n=0.\end{cases}</me>
    Alice grabbed her computer and implemented this in SageMath and computed a few test values.
    </p>
    <sage>
        <input>
def r(n):
    if n == 0:
        return 2
    elif n == 1:
        return 1
    elif n >=2:
        return 2^n + r(n-1)+2*r(n-2)
print(r(1))
print(r(4))
print(r(10))
        </input>
        <output>
1
53
7397            
        </output>
    </sage>
    <p>She then defined a second function <c>s</c> that was the explicit (nonrecursive) solution from <xref ref="ex_recurrence_gf-nonhomog" /> and checked that values matched.</p>
    <sage>
        <input>
s(n)=(5/9)*2^n + (2/3)*n*2^n+(13/9)*(-1)^n
print(s(1))
print(s(4))
print(s(10))
        </input>
        <output>
1
53
7397            
        </output>
    </sage>
    <p><q>Is this going somewhere?</q>, Bob asked impatiently. For these values, both <c>r</c> and <c>s</c> seemed to be giving them answers equally quickly. Dave said he'd heard something about a <c>timeit</c> command in SageMath that would allow them to compare run times and comandeered Alice's keyboard to type:</p>
    <sage>
        <input>
for n in range(31):
    if n % 5 == 0:
        print("For n = {}:".format(n))
        timeit("r(n)",number=5)
        timeit("s(n)",number=5)
        </input>
        <output>
For n = 0:
5 loops, best of 3: 238 ns per loop
5 loops, best of 3: 44 µs per loop
For n = 5:
5 loops, best of 3: 11.4 µs per loop
5 loops, best of 3: 50.6 µs per loop
For n = 10:
5 loops, best of 3: 127 µs per loop
5 loops, best of 3: 47.6 µs per loop
For n = 15:
5 loops, best of 3: 1.42 ms per loop
5 loops, best of 3: 50 µs per loop
For n = 20:
5 loops, best of 3: 15.7 ms per loop
5 loops, best of 3: 49 µs per loop
For n = 25:
5 loops, best of 3: 133 ms per loop
5 loops, best of 3: 50.4 µs per loop
For n = 30:
5 loops, best of 3: 1.49 s per loop
5 loops, best of 3: 48.8 µs per loop
        </output>
    </sage>
    <p>This finally got Bob's attention, since it <c>s</c> seems to be taking a relatively constant time to run even as <m>n</m> increases, while <c>r</c> seems to be taking about 10 times as long to run for each increase of 5 in the value of <m>n</m>. As a final test, they execute the SageMath code below, which calculates <c>s(100)</c> almost instantly. On the other hand, it seems like getting a refill on their coffee would be a good way to pass the time waiting on <c>r(40)</c>.</p>
    <sage>
        <input>
            print(s(100))
            print(r(40))
        </input>
        <output>
85214290348675420878389493250277
29931149867237            
        </output>
    </sage>
</section>
